# 在做下面這段程式碼時，score逐漸攀升過段時間又急遽下跌，之後又逐漸攀升的原因是甚麼呢?
```
python "C:\Users\yuhsu\pysc2-rl-agents\run.py" experiment_03 --agent feudal --map MoveToBeacon --envs 4 --res 32 --steps_per_batch 32 --iters 100000 --lr 0.0005 --entropy_weight 0.01 --save_iters 2000 --summary_iters 50 --vis --value_loss_weight 0.7 --discount 0.99
```

# ANS
這是一種常見的強化學習現象，可能有以下幾個原因：
## 1. 探索與利用的平衡
* 強化學習的過程中，Agent 需要在「探索新策略」和「利用已知策略」之間做權衡。
* 初期，Agent 可能會學到一些不錯的策略，使分數上升，但當它繼續探索新策略時，可能會嘗試不同的方法，導致短期內表現變差（分數下降）。
* 之後，隨著訓練的進行，它可能會找到更好的策略，讓分數再次回升。

## 2. 梯度爆炸或消失
* 模型使用的是 RMSProp 優化器，並且有梯度裁剪 (`clip_gradients=max_gradient_norm`)，這可以在一定程度上防止梯度爆炸，但不一定能完全避免。
* 如果梯度變得過大，訓練可能會變得不穩定，導致策略出現劇烈變化，進而讓分數突然下降。

## 3. 熵權重 (entropy_weight) 的影響
* 程式碼的 `entropy_weight = 0.01`，這代表模型會鼓勵探索新策略。
* 熵（Entropy）用來讓 Agent 嘗試不同的行動，但如果熵權重過高，Agent 可能會一直嘗試新策略，而不是專注於已學會的好策略，這可能導致分數下降。
* 可以試著降低 `entropy_weight`（例如 `0.005`）看看是否能讓分數更穩定。

## 4. 學習率 (learning rate) 影響
* 程式碼的學習率 lr = 0.0005，但程式碼中有這一行:
  ```
  learning_rate = tf.train.exponential_decay(learning_rate, global_step, 10000, 0.94)
  ```
  這表示每 10,000 步，學習率會衰減為原本的 94%，這樣可以讓訓練後期變得穩定。
* 但是，如果學習率太快下降，模型可能無法適應新的情境，導致分數下降。
* 可以調整 `decay_rate`（例如 0.98 而不是 0.94），讓學習率下降得更慢。

## 5. 內在獎勵 (Intrinsic Reward) 的影響
*  Feudal Agent 包含 Manager（管理者） 和 Worker（工人），而 `RI`（內在獎勵）是 Worker 的學習目標之一
  ```
  worker_value_loss = tf.reduce_mean(tf.square(RI - train_model.value[1])) / 2
  ```
* 如果內在獎勵設計不當，Worker 可能會學到對整體目標不利的行為，導致分數下降

## 6. 批次大小 (Batch Size)
* `steps_per_batch = 32`，這代表每次更新參數時會用 32 個 step 的資料。
* 如果批次大小太小，更新時會有較大的變動，可能導致訓練不穩定。
* 可以試著增加 `steps_per_batch`（例如 64）來減少波動


# 建議的嘗試方向
## 1. 調整熵權重
* 嘗試降低 `entropy_weight`（例如 `0.005`），讓 Agent 在學到有效策略後不會過度探索。

## 2. 調整學習率衰減
* 減緩學習率衰減速度，將 `decay_rate` 改為 `0.98`，避免學習率過快下降導致模型無法適應新策略。

## 3. 增加批次大小
* 嘗試把 `steps_per_batch` 增加到 64，減少梯度更新時的隨機波動，使訓練更穩定。

## 4. 觀察內在獎勵 (RI) 的影響
* 在 TensorBoard 或 Log 中查看 `RI`（內在獎勵）是否有異常波動，確保 Worker 的學習方向正確。


這些調整可能會幫助 Feudal Agent 更穩定地學習，避免劇烈的分數波動。








